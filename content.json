{"meta":{"title":"木子李的小屋","subtitle":"我将忠于自己, 披星戴月, 奔向理想和你！","description":"机器学习菜鸟","author":"LHH","url":"https://lhhcoding.top","root":"/"},"pages":[{"title":"本文章由木子李编写，如有疑问欢迎讨论指正。","date":"2021-05-09T09:53:57.000Z","updated":"2021-05-20T09:08:50.610Z","comments":true,"path":"about/index.html","permalink":"https://lhhcoding.top/about/index.html","excerpt":"","text":""},{"title":"im","date":"2021-05-20T10:16:25.000Z","updated":"2021-05-20T10:16:25.149Z","comments":true,"path":"im/index.html","permalink":"https://lhhcoding.top/im/index.html","excerpt":"","text":""},{"title":"guestbook","date":"2021-05-11T08:21:35.000Z","updated":"2021-05-12T05:50:32.587Z","comments":true,"path":"guestbook/index.html","permalink":"https://lhhcoding.top/guestbook/index.html","excerpt":"","text":"欢迎来到我的博客！ 欢迎在这里留言！任何问题都可以在这里留言，我会及时回复的，添加email可以获得更快的回复速度，在nickname栏目输入QQ号可以直接获取你的QQ头像。"},{"title":"友情链接","date":"2021-05-19T14:44:45.000Z","updated":"2021-05-19T14:49:10.054Z","comments":true,"path":"link/index.html","permalink":"https://lhhcoding.top/link/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-05-09T09:52:56.000Z","updated":"2021-05-12T05:50:32.588Z","comments":false,"path":"tags/index.html","permalink":"https://lhhcoding.top/tags/index.html","excerpt":"","text":""},{"title":"文章分类","date":"2018-10-14T16:03:57.000Z","updated":"2021-05-12T05:50:32.587Z","comments":false,"path":"categories/index.html","permalink":"https://lhhcoding.top/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"markdown语法记录","slug":"markdown语法记录","date":"2021-05-14T15:07:01.000Z","updated":"2021-05-20T12:38:43.560Z","comments":true,"path":"archives/2a06da23.html","link":"","permalink":"https://lhhcoding.top/archives/2a06da23.html","excerpt":"markdown 语法速查记录","text":"markdown 语法速查记录 缩进、空格 = 2个 = 4个 文本加粗 wenben or wenben 斜体 斜体 删除线 删除线 下划线 下划线 (后面记得加斜杠) 文本高亮 高亮 $$ s^2 $$ $$ \\frac{1}{m} \\sum_{i=1}^n|y-y_i| $$","categories":[{"name":"markdown","slug":"markdown","permalink":"https://lhhcoding.top/categories/markdown/"}],"tags":[{"name":"markdown","slug":"markdown","permalink":"https://lhhcoding.top/tags/markdown/"}]},{"title":"Hello World","slug":"hello-world","date":"2021-05-14T14:20:00.163Z","updated":"2021-05-14T14:20:00.163Z","comments":true,"path":"archives/4a17b156.html","link":"","permalink":"https://lhhcoding.top/archives/4a17b156.html","excerpt":"开始","text":"开始 Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new &quot;My New Post&quot; More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"回归模型中特征重要性选择","slug":"回归模型中特征重要性选择","date":"2021-05-14T07:23:23.000Z","updated":"2021-05-18T07:47:01.370Z","comments":true,"path":"archives/6fd9971a.html","link":"","permalink":"https://lhhcoding.top/archives/6fd9971a.html","excerpt":"回归模型中特征选择方法","text":"回归模型中特征选择方法 协方差对于单变量选择 123456789101112# 计算变量的方差# 如果方差接近于0，也就是该特征的特征值之间基本上没有差异，这个特征对于样本的区分并没有什么用，剔除from sklearn.feature_selection import VarianceThresholdselector &#x3D; VarianceThreshold(threshold&#x3D;0.1) #默认threshold&#x3D;0.0selector.fit_transform(df[numerical_features])# 查看各个特征的方差,selector.variances_ ,len(selector.variances_)# 特征对应方差all_used_features_dict &#x3D; dict(zip(numerical_features,selector.variances_ ))all_used_features_dict 数值特征和目标值之间相关性 1. 协方差 如果协方差为正，说明X,Y同向变化，协方差越大说明同向程度越高； 如果协方差维负，说明X，Y反向运动，协方差越小说明反向程度越高； 如果两个变量相互独立，那么协方差就是0，说明两个变量不相关。 2. pearson系数 2.1 相关概念和值大小含义 相关系数也可以看成协方差：一种剔除了两个变量量纲影响、标准化后的特殊协方差。 可以反映两个变量变化时是同向还是反向，如果同向变化就为正，反向变化就为负。由于它是标准化后的协方差，因此更重要的特性来了，它消除了两个变量变化幅度的影响，而只是单纯反应两个变量每单位变化时的相似程度。 假设对于Pearson r相关性，两个变量都应该是正态分布的 pearson数值大小衡量相关性： 0.8-1.0 极强相关 | 0.6-0.8 强相关 | 0.4-0.6 中等程度相关 | 0.2-0.4 弱相关 | 0.0-0.2 极弱相关或无相关 2.2 pearson 系数的优缺点： 优点： 可以通过数值对变量之间相关性衡量，正值代表正相关、负值代表负相关、0代表不相关 缺点： 没有对变量之间的关系进行提炼和学习，预测其实是学习不同特征之间的组合既关系。只能判别特征之间的线性相关性，如果是非线性相关就不可取。 2.3 适用场景 两个变量之间是线性关系，都是连续数据。 两个变量的总体是正态分布，或接近正态的单峰分布。 两个变量的观测值是成对的，每对观测值之间相互独立。 2.4 相关代码 通过numpy 12import numpy as npnp.corrcoef([a,b,c,d]) pandas中corr()函数 12345678import matplotlib.pyplot as pltplt.figure(figsize &#x3D; (25,25))#### 传入相关特征即可，输出为所有特征之间相关性corr_values1 &#x3D; data[features].corr()sns.heatmap(corr_values1, annot&#x3D;True,vmax&#x3D;1, square&#x3D;True, cmap&#x3D;&quot;Blues&quot;,fmt&#x3D;&#39;.2f&#39;)plt.tight_layout()plt.savefig(&#39;**.png&#39;,dpi&#x3D;600)plt.show() 利用scipy，输出两个值，第一个值为相关系数，第二个值越小代表两个之间相关性越高 1234import numpy as npfrom scipy.stats import pearsonr### 计算两个特征之间相关性，同时也可以计算特征和标签之间相关性print(&quot;Lower noise&quot;, df(x, x1)) 计算每个特征和label之间相关性 12345678# drop_list 为不需要计算的特征或者可以是类别变量特征feature_list &#x3D; [col for col in df.columns if col not in drop_list]feature_import_df &#x3D; pd.DataFrame()for i,f in enumerate(feature_list): feature_import_df.loc[i,&#39;feature_name&#39;] &#x3D; f feature_import_df.loc[i,&#39;pearson&#39;] &#x3D; np.corroef(df[label],df[f])### 排序看特征和label之间相关性feature_import_df.sort_values(by&#x3D;[&#39;pearson&#39;],inplace &#x3D; True) 2.5 通过pearson系数删选特征 (1) 通过和label之间的相关性之间，通过设置阈值删选 12345678910111213141516def del_corr_fea(df,cor_df): &quot;&quot;&quot; df是原始数据，cor_df为通过pd.corr()获得特征间相关性矩阵， &quot;&quot;&quot; cor_df &#x3D; cor_df.reset_index() feature_col &#x3D; [col for col in df.columns if col not in drop_fea_list] drop_fea &#x3D; [] for i,f in enumerate(feature_col): if f not in drop_fea: cor_df1 &#x3D; cor_df[i+1:][[f,&#39;index&#39;]] cor_df_sel &#x3D; cor_df1[cor_df1[f]&gt;&#x3D;0.8] cor_df_sel.sort_values(by&#x3D;[f],ascending &#x3D; False,inplace &#x3D; True) del_name &#x3D; cor_df_sel[&#39;index&#39;].values.tolist()[1:] drop_fea &#x3D; del_name + drop_fea return drop_feadrop_list_no_p &#x3D; del_corr_fea(data_end,corr_values_fea_fea) (2) 首先计算不同特征之间相关性，然后通过相关性取出相似性最高的几个特征，并保留和label间系数最高的特征 123456789101112131415161718192021def del_corr_fea(df,cor_df,cor_df_with_label): &quot;&quot;&quot; df是原始数据，cor_df为通过pd.corr()获得特征间相关性矩阵，cor_df_with_label和标签之间相关性 &quot;&quot;&quot; cor_df &#x3D; cor_df.reset_index() cor_df &#x3D; cor_df.rename(columns &#x3D; &#123;&#39;index&#39;:&#39;feature&#39;&#125;) feature_col &#x3D; [col for col in df.columns if col not in drop_fea_list] drop_fea &#x3D; [] for i,f in enumerate(feature_col): if f not in drop_fea: print(len(drop_fea)) cor_df1 &#x3D; cor_df[i:][[f,&#39;feature&#39;]] cor_df_sel &#x3D; cor_df1[cor_df1[f]&gt;&#x3D;0.8] sort_corr_df &#x3D; cor_df_sel.merge(cor_df_with_label,on &#x3D; &#39;feature&#39;,how &#x3D; &#39;left&#39;) ## p 更改为相关性矩阵的列名 sort_corr_df.sort_values(by&#x3D;[&#39;p&#39;],ascending &#x3D; False,inplace &#x3D; True) del_name &#x3D; sort_corr_df[&#39;feature&#39;].values.tolist()[1:] drop_fea &#x3D; del_name + drop_fea return drop_feadrop_feature_list &#x3D; del_corr_fea(data_end,corr_values_fea_fea,d_df)len(drop_feature_list) 通过模型输出特征重要性（包括数值型和类别） 最简单方式（回归模型，分类可以去官网查 https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest.set_params） 12345678910111213from sklearn.feature_selection import SelectKBest,f_regressionprint(data_end.shape)sk&#x3D;SelectKBest(f_regression,k&#x3D;300)# drop_columns 为不需要判别的列名new_train&#x3D;sk.fit_transform(data_end.drop(drop_columns,axis &#x3D; 1),data_end[&#39;label&#39;].astype(&#39;int&#39;))print(new_train.shape)# 获取对应列索引select_columns&#x3D;sk.get_support(indices &#x3D; True)print(select_columns)print(data_end.columns[select_columns]) 通过树模型输出特征重要性，一般选用Xgboost、Lightgbm等，这里采用lightgbm示例。 1234567891011121314151617181920212223242526import warningswarnings.filterwarnings(&#39;ignore&#39;)from lightgbm.sklearn import LGBMRegressorX_train,y_train &#x3D; train.drop([&#39;label&#39;],axis &#x3D; 1),train[&#39;label&#39;]other_params &#x3D; &#123;&#39;boosting_type&#39;:&#39;gbdt&#39;,&#39;colsample_bytree&#39;:0.6, &#39;importance_type&#39;:&#39;split&#39;,&#39;learning_rate&#39;:0.06999999999999999, &#39;max_depth&#39;:35, &#39;metric&#39;:&#39;mape&#39;,&#39;min_child_samples&#39;:30, &#39;min_child_weight&#39;:0.001, &#39;min_split_gain&#39;:0.0,&#39;n_estimators&#39;:1100, &#39;n_jobs&#39;:-1, &#39;num_leaves&#39;:45, &#39;objective&#39;:&#39;mape&#39;,&#39;random_state&#39;:2020, &#39;reg_alpha&#39;:0.2, &#39;reg_lambd&#39;:0.1, &#39;reg_lambda&#39;:0.4,&#39;silent&#39;:True, &#39;subsample&#39;:0.9999999999999999,&#39;subsample_for_bin&#39;:200000, &#39;subsample_freq&#39;:0&#125;model &#x3D; LGBMRegressor(**other_params)model.fit(X_train,y_train) feature_names &#x3D; list(X_train)feature_df &#x3D; pd.DataFrame(&#123; &#39;column&#39;: feature_names, &#39;importance&#39;: model.feature_importances_, &#125;).sort_values(by&#x3D;&#39;importance&#39;,ascending &#x3D; False)## 得到前一百个特征feature_df[:100]### 最终根据特征重要性筛选出前面特征，也可以根据特征重要性看是否和自己做特征工程的想法是否一致。","categories":[{"name":"回归预测","slug":"回归预测","permalink":"https://lhhcoding.top/categories/%E5%9B%9E%E5%BD%92%E9%A2%84%E6%B5%8B/"}],"tags":[{"name":"特征选择","slug":"特征选择","permalink":"https://lhhcoding.top/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"}]},{"title":"pandas总结","slug":"pandas总结","date":"2021-05-13T08:59:18.000Z","updated":"2021-05-14T14:20:00.164Z","comments":true,"path":"archives/b8b772a5.html","link":"","permalink":"https://lhhcoding.top/archives/b8b772a5.html","excerpt":"","text":"pandas日常使用总结 pandas 中没有获取众数 思路：通过统计次数，并且获取到最大次数即为众数 1data[[&#39;diff_days&#39;,&#39;cust_no&#39;]].groupby(&#39;cust_no&#39;).agg(lambda x: x.value_counts().index[0]).reset_index()","categories":[{"name":"日常总结","slug":"日常总结","permalink":"https://lhhcoding.top/categories/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"https://lhhcoding.top/tags/pandas/"}]},{"title":"hexo-换电脑更新博客","slug":"hexo-换电脑更新博客","date":"2021-05-12T03:18:03.000Z","updated":"2021-05-14T14:20:00.164Z","comments":true,"path":"archives/397eb78d.html","link":"","permalink":"https://lhhcoding.top/archives/397eb78d.html","excerpt":"开始","text":"开始 步骤：在原电脑上操作，给 username.github.io 博客仓库创建hexo分支，并设为默认分支。（具体可参考这篇文章的操作，有图示）如果未给你的 github 账号添加过当前电脑生成的 ssh key，需要创建 ssh key 并添加到 github 账号上。（如何创建和添加 github help 就有）随便一个目录下，命令行执行 git clone git@github.com:username/username.github.io.git 把仓库 clone 到本地。显示所有隐藏文件和文件夹，进入刚才 clone 到本地的仓库，删掉除了 .git 文件夹以外的所有内容。命令行 cd 到 clone 的仓库，git add -A ，git commit -m “–”，git push origin hexo，把刚才删除操作引起的本地仓库变化更新到远程，此时刷新下 github 端博客hexo分支，应该已经被清空了。将上述 .git 文件夹复制到本机本地博客根目录下（即含有 themes、source 等文件夹的那个目录），现在可以把上述 clone 的本地仓库删掉了，因为它已经没有用了，本机博客目录已经变成可以和 hexo 分支相连的仓库了。将博客目录下 themes 文件夹下每个主题文件夹里面的 .git .gitignore 删掉。 cd 到博客目录，git add -A ，git commit -m “–”，git push origin hexo，将博客目录下所有文件更新到 hexo 分支。如果上一步没有删掉 .git .gitignore，主题文件夹下内容将传不上去。至此原电脑上的操作结束。在新电脑上操作，先把新电脑上环境安装好，node.js、git、hexo，ssh key 也创建和添加好。选好博客安装的目录， git clone git@github.com:username/username.github.io.git 。cd 到博客目录，npm install、hexo g &amp;&amp; hexo s，安装依赖，生成和启动博客服务。正常的话，浏览器打开 localhost:4000 可以看到博客了。至此新电脑操作完毕。以后无论在哪台电脑上，更新以及提交博客，依次执行，git pull，git add -A ，git commit -m “–”，git push origin hexo，hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 即可。 链接：https://www.zhihu.com/question/21193762/answer/489124966 更换电脑操作一样的，跟之前的环境搭建一样，安装gitsudo apt-get install git 设置git全局邮箱和用户名git config --global user.name “yourgithubname” git config --global user.email “yourgithubemail” 设置ssh keyssh-keygen -t rsa -C “youremail” #生成后填到github和coding上（有coding平台的话） #验证是否成功 ssh -T git@github.com ssh -T git@git.coding.net #(有coding平台的话) 安装nodejssudo apt-get install nodejs sudo apt-get install npm 安装hexo sudo npm install hexo-cli -g 但是已经不需要初始化了，直接在任意文件夹下，git clone git@……………… 然后进入克隆到的文件夹：cd xxx.github.io npm install npm install hexo-deployer-git --save 生成，部署：hexo g hexo d 然后就可以开始写你的新博客了hexo new newpage Tips:不要忘了，每次写完最好都把源文件上传一下git add . git commit –m “xxxx” git push 如果是在已经编辑过的电脑上，已经有clone文件夹了，那么，每次只要和远端同步一下就行了git pull","categories":[],"tags":[]},{"title":"next主题配置","slug":"next主题配置","date":"2021-05-10T08:59:14.000Z","updated":"2021-05-12T05:50:32.585Z","comments":true,"path":"archives/c67bcb84.html","link":"","permalink":"https://lhhcoding.top/archives/c67bcb84.html","excerpt":"","text":"","categories":[],"tags":[]},{"title":"lightgbm","slug":"lightgbm","date":"2021-04-28T08:10:07.000Z","updated":"2021-05-17T16:04:00.545Z","comments":true,"path":"archives/8df69e6b.html","link":"","permalink":"https://lhhcoding.top/archives/8df69e6b.html","excerpt":"Lightgbm 获取特征重要性","text":"Lightgbm 获取特征重要性 lightgbm主要分为两种方式训练，一种是通过原生API另外一种是通过scikit-learn中python API 方式 该列是主要的特征 点击跳转至百度 123456feature_df = pd.DataFrame(&#123; &#x27;column&#x27;: feature_names, &#x27;importance&#x27;: lgb_model.feature_importances_, &#125;).sort_values(by=&#x27;importance&#x27;,ascending = False)## 得到前一百个特征feature_df[:100] 回归评价指标和相关理解 回归预测评价指标对比 $ y_i 代表预测结果，y 代表真实值 $。 MAE $$ \\frac{1}{m} \\sum_{i=1}^n|y-y_i| $$ 衡量真实值和预测值之间的差距，并且得到的是一个平均值，MAE二阶不可导，在xgboost、lightgbm中使用近似函数代替。 MSE $$ \\frac{1}{m} \\sum_{i=1}^n(y-y_i)^2 $$ 比MAE得到的值会更大，因为有一个平方，差值大于一会成倍放大，会更加倾向更新差值较大的数据。 RMSE $$ \\sqrt{\\frac{1}{m} \\sum_{i=1}^n(y-y_i)^2} $$ 和MSE类似，只不过将结果缩放到和原数据同等数量级上。 MAPE $$ \\frac{1}{m} \\sum_{i=1}^n|\\frac{y-y_i}{y}| $$ 用来衡量误差和占真实值比例，这也是在实际中常见的评估方式。 SMPE $$ \\frac{100%}{m}\\sum_{i=1}^n\\frac{|y - y_i|}{(|y|+|y_i|)/2}$$ 该损失函数是为了将MAPE中更加倾向优化小值的情况。 R2-score $$ R^2 = 1 - \\frac{\\sum_i{(y-y_i)^2}}{\\sum_i{(\\bar{y} - y_i)^2}} $$ 该系数的取值范围为$(-\\infty) - 0$,最好结果是1，预测值和真实值完全拟合。如果预测结果还没有使用平均值结果好就会达到小于零的值。 矫正决定系数 $$ R^2_{ad} = 1 - \\frac{(1-R^2)(m-1)}{m-p-1} $$ 其中m是样本数量，p是特征数量 123456789from sklearn.metrics import mean_squared_error #均方误差from sklearn.metrics import mean_absolute_error #平方绝对误差from sklearn.metrics import r2_score#R square#调用MSE：mean_squared_error(y_test,y_predict)RMSE:np.sqrt(mean_squared_error(y_test,y_predict))MAE：mean_absolute_error(y_test,y_predict)R2：r2_score(y_test,y_predict)Adjusted_R2：:1-((1-r2_score(y_test,y_predict))*(m-1))/(m-p-1) 思考如何评价一个模型是否拟合的好？ 假设有五个样本标签为 {10,15,20,13,18} 预测结果为 {9,17,18,10,16} 不同指标结果如下： MAE：2 MSE:4.4 RMSE: $\\sqrt{4.4}$ MAPE: (0.1 + 2/15+2/20+3/13+2/18)/5 = 0.135 SMAPE: (21 / (10+9) + 22/(15+17) + 22/(20+18) + 23/(13+10) + 2*2/(16+18))/5 = 0.143 R2：0.65 R2-score: 这块没有特征的数量没有计算，在实际训练中该值是确定的，取决于特征数量 RMSE 值比MA值大的原因是因为大于一的值会放大，解释RMSE为什么会对异常值更加敏感, 也会有小于1的值会被缩小，但是小于1的值缩小值远不及放大的值。假设两个样本：真实值{10,15}，预测值{9.5,10} MAE：2.75 RMSE: 5.05 为什么MAE倾向于拟合平均值？RMSE倾向于中位数？MAPE对小值更加敏感？ 为什么MAE倾向于拟合平均值？ RMSE倾向于拟合中位数？ MAPE对小值更加敏感？ 怎么对比不同时间序列的预测准确性？ 首先不同时间序列真实值并不是完全一致的。比如A的时间序列平均值为100，B的时间序列的平均值为10000，两个时间序列在量纲上存在差异，如果直接使用MAE，MSE, RMSE等指标会存在明显差异。 可以考虑将目标值最大最小变化到同一量纲上面 比较相同时间序列在不同模型中差异 可以比较在不同模型下预测结果值的大小","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://lhhcoding.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"回归评测指标","slug":"回归评测指标","permalink":"https://lhhcoding.top/tags/%E5%9B%9E%E5%BD%92%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87/"}]}],"categories":[{"name":"markdown","slug":"markdown","permalink":"https://lhhcoding.top/categories/markdown/"},{"name":"回归预测","slug":"回归预测","permalink":"https://lhhcoding.top/categories/%E5%9B%9E%E5%BD%92%E9%A2%84%E6%B5%8B/"},{"name":"日常总结","slug":"日常总结","permalink":"https://lhhcoding.top/categories/%E6%97%A5%E5%B8%B8%E6%80%BB%E7%BB%93/"},{"name":"机器学习","slug":"机器学习","permalink":"https://lhhcoding.top/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"markdown","slug":"markdown","permalink":"https://lhhcoding.top/tags/markdown/"},{"name":"特征选择","slug":"特征选择","permalink":"https://lhhcoding.top/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"},{"name":"pandas","slug":"pandas","permalink":"https://lhhcoding.top/tags/pandas/"},{"name":"回归评测指标","slug":"回归评测指标","permalink":"https://lhhcoding.top/tags/%E5%9B%9E%E5%BD%92%E8%AF%84%E6%B5%8B%E6%8C%87%E6%A0%87/"}]}